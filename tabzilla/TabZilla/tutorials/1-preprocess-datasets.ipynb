{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: TabZilla Datasets\n",
    "\n",
    "This notebook demonstrates how to use TabZilla to download & preprocess datasets for analysis.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. You need to have a python environment with the following python packages. We recommend following instructions on our [README](https://github.com/naszilla/tabzilla/blob/main/README.md) to prepare a virtual environment with `venv`. Required packages:\n",
    "\n",
    "- [`openml`](https://pypi.org/project/openml/)\n",
    "- [`argparse`](https://pypi.org/project/argparse/)\n",
    "- [`pandas`](https://pypi.org/project/pandas/)\n",
    "- [`scikit-learn`](https://pypi.org/project/scikit-learn/)\n",
    "\n",
    "2. Make an [OpenML](www.openml.org) account. You might need to authenticate your account using an API key. \n",
    "\n",
    "3. Like all of our code, this notebook must be run from the TabZilla directory. Make sure to run the following cell to `cd` one level up, by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/duncan/research/active_projects/tabzilla/TabZilla\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you can import the openml package\n",
    "import openml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you can't import `openml`\n",
    "\n",
    "Please read [this guide](https://docs.openml.org/Python-guide/), and make sure you can import `openml` before proceeding.\n",
    "\n",
    "### If you *can* import `openml`!\n",
    "\n",
    "Use TabZilla code to pre-process a dataset! To pre-process a dataset, you need to pass a valid dataset name, using the TabZilla naming convention: `openml_<dataset-name>__<dataset-id>`. The code below prepares the OpenML Audiology dataset, and writes it to the folder `TabZilla/datasets`. Since we set flag \"overwrite=False\", we will check first whether the dataset has already been pre-processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duncan/research/python_venv/tabzilla/lib/python3.10/site-packages/openml/tasks/functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "/Users/duncan/research/python_venv/tabzilla/lib/python3.10/site-packages/openml/datasets/functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openml__audiology__7                    | Found existing folder. Skipping.\n"
     ]
    }
   ],
   "source": [
    "from tabzilla_data_preprocessing import preprocess_dataset\n",
    "\n",
    "dataset_path = preprocess_dataset('openml__audiology__7', overwrite=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which datasets can I pre-process with TabZilla?\n",
    "\n",
    "To see a list of all valid TabZilla dataset names, look at the keys of `tabzilla_data_preprocessing.preprocessors`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openml__sick__3021\n",
      "openml__kr-vs-kp__3\n",
      "openml__letter__6\n",
      "openml__balance-scale__11\n",
      "openml__mfeat-factors__12\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from tabzilla_data_preprocessing import preprocessors\n",
    "\n",
    "valid_dataset_names = list(preprocessors.keys())\n",
    "\n",
    "for n in valid_dataset_names[:5]: \n",
    "    print(n)\n",
    "\n",
    "print(\"...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the pre-processed dataset\n",
    "\n",
    "Now that we prepared the Audiology dataset, we can read it using the TabZilla dataset interface. To read pre-processed datasets, you need to pass the local path where the dataset is stored. Our pre-processing code writes pre-processed datasets to `TabZilla/datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabzilla_datasets import TabularDataset\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = TabularDataset.read(Path(\"./datasets/openml__audiology__7\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset object contains lots of useful information, such as the number of categorical features, the dataset size, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TabularDataset in module tabzilla_datasets:\n",
      "\n",
      "class TabularDataset(builtins.object)\n",
      " |  TabularDataset(name: str, X: numpy.ndarray, y: numpy.ndarray, cat_idx: list, target_type: str, num_classes: int, num_features: Optional[int] = None, num_instances: Optional[int] = None, cat_dims: Optional[list] = None, split_indeces: Optional[list] = None, split_source: Optional[str] = None) -> None\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, name: str, X: numpy.ndarray, y: numpy.ndarray, cat_idx: list, target_type: str, num_classes: int, num_features: Optional[int] = None, num_instances: Optional[int] = None, cat_dims: Optional[list] = None, split_indeces: Optional[list] = None, split_source: Optional[str] = None) -> None\n",
      " |      name: name of the dataset\n",
      " |      X: matrix of shape (num_instances x num_features)\n",
      " |      y: array of length (num_instances)\n",
      " |      cat_idx: indices of categorical features\n",
      " |      target_type: {\"regression\", \"classification\", \"binary\"}\n",
      " |      num_classes: 1 for regression 2 for binary, and >2 for classification\n",
      " |      num_features: number of features\n",
      " |      num_instances: number of instances\n",
      " |      split_indeces: specifies dataset splits as a list of dictionaries, with entries \"train\", \"val\", and \"test\".\n",
      " |          each entry specifies the indeces corresponding to the train, validation, and test set.\n",
      " |  \n",
      " |  cat_feature_encode(self)\n",
      " |  \n",
      " |  get_metadata(self) -> dict\n",
      " |  \n",
      " |  target_encode(self)\n",
      " |  \n",
      " |  write(self, p: pathlib.Path, overwrite=False) -> None\n",
      " |      write the dataset to a new folder. this folder cannot already exist\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  read(p: pathlib.Path) from builtins.type\n",
      " |      read a dataset from a folder\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name: openml__audiology__7\n",
      "target type: classification\n",
      "number of target classes: 24\n",
      "number of features: 69\n",
      "number of instances: 226\n",
      "indices of categorical features: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset name: {dataset.name}\")\n",
    "print(f\"target type: {dataset.target_type}\")\n",
    "print(f\"number of target classes: {dataset.num_classes}\")\n",
    "print(f\"number of features: {dataset.num_features}\")\n",
    "print(f\"number of instances: {dataset.num_instances}\")\n",
    "print(f\"indices of categorical features: {dataset.cat_idx}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is the acutal data?\n",
    "\n",
    "All features are stored in attirbute `X`, and targets are stored in attribute `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (226, 69)\n",
      "y.shape: (226,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X.shape: {dataset.X.shape}\")\n",
    "print(f\"y.shape: {dataset.y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0, :]:\n",
      "[1 0 0 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 1 0 0 0 0]\n",
      "y[0]:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# first instance in the dataset:\n",
    "print(\"X[0, :]:\")\n",
    "print(dataset.X[0, :])\n",
    "\n",
    "# first target:\n",
    "print(\"y[0]:\")\n",
    "print(dataset.y[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset splits\n",
    "\n",
    "To maintain consistency between experiments, we used fixed dataset splits defined in the OpenML task. These splits are also defined in the dataset object. The attribute `dataset.split_indeces` is a list of 10 dictionaries, each containing the indices of train, test, and validation instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([ 96, 163, 200,  70,  38,  31,  30,  37,  98,  20, 170, 112, 138,\n",
       "         78, 132, 119,  24,  47, 141, 108, 123,  28, 143,  34, 208, 158,\n",
       "        206,  73, 196, 203, 155,  84, 189, 152, 166, 182,  57, 171, 201,\n",
       "        184,  94,  72, 137, 114, 202,  54, 150, 118, 205,  27,  61, 225,\n",
       "         40, 168,  41, 125, 221, 116, 101, 133, 177, 178, 165, 154, 160,\n",
       "        107, 211, 135, 120,  58, 180,  45,  55,  32, 153, 161,  99,  35,\n",
       "         36, 144, 145, 115, 162,  56,  85, 218,  88, 159,  17, 121,  71,\n",
       "        142, 105,  82,  33, 136, 173, 109,   0, 187,  68, 188,  29, 223,\n",
       "         97, 176, 169, 172,  62,  87, 214,  23, 127, 134, 131, 207,  95,\n",
       "        149,  52,  83,  18, 157, 209, 181, 140,  16,  39,  63,   8,  48,\n",
       "         25,   2, 156,  10, 106,  64, 213, 197, 199,  91,   1,  66, 147,\n",
       "          7, 148,  15, 222,  67,  49,  44, 151,  13, 179,  93, 215, 117,\n",
       "         60,  79,  26, 198, 124,  59, 130,  14,  65,   4, 190,   6, 126,\n",
       "        216,  90,   9,  53, 183, 100,  46, 192, 175, 110, 102], dtype=int32),\n",
       " 'test': array([174,  81,  51,  69, 146, 224, 195, 219, 220, 210,   5, 128,  92,\n",
       "         22,  12,  86,   3, 217, 185, 191, 139, 111, 104], dtype=int32),\n",
       " 'val': array([ 89, 194, 212,  50, 193,  75,  19, 167, 204,  76,  77, 113,  74,\n",
       "         80,  42,  43, 186, 164, 129,  11, 122,  21, 103], dtype=int32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.split_indeces[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Datasets for ML pipelines\n",
    "\n",
    "The pre-processing described above simply downloads the OpenML datasets and saves them in a TabZilla-readable format. Before passing these datasets into an ML pipeline, we need to run some additional processing steps, including scaling & cleaning the features, encoding categorical features, and encoding the target. All of these are handled by the function [`tabzilla_data_processing.process_data'](tabzilla/blob/main/TabSurvey/tabzilla_data_processing.py).\n",
    "\n",
    "The function `process_data` takes as input the indices of all training, testing, and validation instances, which can be read from the dataset attribute `split_indeces`. \n",
    "\n",
    "Here is an example using our pre-processed Audiology dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabzilla_data_processing import process_data\n",
    "\n",
    "processed_dataset_fold0 = process_data(\n",
    "    dataset,\n",
    "    dataset.split_indeces[0]['train'],\n",
    "    dataset.split_indeces[0]['val'],\n",
    "    dataset.split_indeces[0]['test'],\n",
    "    impute=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed dataset object contains (X, y) pairs for the training, testing, and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 2, 0, ..., 0, 0, 0],\n",
      "       [0, 2, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 2, 0, ..., 0, 0, 0]], dtype=object), array([ 7,  7,  7,  7,  7,  2,  2,  2,  2,  2,  2, 19, 18, 18,  6,  6,  3,\n",
      "        3,  3, 14, 22, 17,  5]))\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset_fold0[\"data_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabzilla",
   "language": "python",
   "name": "tabzilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
